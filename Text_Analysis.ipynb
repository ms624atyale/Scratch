{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM/+P85SpP073WB6m6IBTYd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms624atyale/Scratch/blob/main/Text_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🐹 🐾 [Text Corpus <font size='1.8'>코퍼스/말뭉치</font>](https://en.wikipedia.org/wiki/Text_corpus)  \n",
        "- In linguistics, a corpus (plural corpora) or text corpus is a language resource consisting of a large and structured set of texts (nowadays usually electronically stored and processed). In corpus linguistics, they are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.\n",
        "\n",
        "- The **corpus-toolkit** package grew out of courses in corpus linguistics and learner corpus research. The toolkit attempts to balance simplicity of use, broad application, and scalability. Common corpus analyses such as the <font color = 'pink'>_calculation of word and n-gram frequency and range, keyness, and collocation_</font> are included. In addition, more advanced analyses such as the identification of <font color = 'pink'>_dependency bigrams (e.g., verb-direct object combinations) and their frequency, range, and strength of association_</font>  are also included.(https://pypi.org/project/corpus-toolkit/)\n",
        "\n",
        "Some conditions should be fulfilled if you want to conduct corpus-related analysis. \n",
        "\n",
        ">1. Read and write a file using an operating system package.\n",
        "\n",
        "🆘 Install the **[os](https://docs.python.org/3/library/os.html)** package.\n",
        "\n",
        ">2. Text files you want to analyze (e.g., url(uniform resource locator) with html document, text files under the Files dicrectory of Google Colab).\n",
        ">3. Text ➡️ Words: **Tokenization**\n",
        ">4. Words with the conjugation, inflection, derivation process ↔️ Words sorted by grouping inflected or variant forms of the same word (i.e., **lemmatization**)\n",
        ">5. POS (part of speech (e.g., word-grammatical category pairs))\n",
        "\n",
        "\n",
        "🆘 Install **corpus-toolkit** and **nltk**(natural language tool kit) packages.\n",
        " \n"
      ],
      "metadata": {
        "id": "M7afPLwh-bRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Download the os package \n",
        "#@markdown 📎 Open a txt file you want to analyze.\n",
        "import os\n",
        "#?os.mkdir(\"txtdata\")"
      ],
      "metadata": {
        "id": "J-ZgsoO2LnYg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBz4z96Z-YV5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown 📌 Download the corpus-toolkit package\n",
        "!pip install corpus-toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-processing (사전처리)\n",
        "- 코퍼스 분석을 하려면 분석하려는 텍스트의 단어를 토큰화하여 분석할 수 있는 형태로 준비해 놓아야 한다. i) tokenize 함수를 사용하여 토큰을 리스트화 하여 준비해 놓을 수도 있고, ii) tag 함수를 사용하여 토큰에 문법범주(품사)를 연결해서 준비해 놓을 수도 있다.  \n",
        "-- [코드문법1] corpus_toolkit패키지에서 corpus_tools모듈을 ct로 줄여서 불러들여라.\n",
        "-- [코드문법2] [파일 올리고 읽기] ct모듈의 ldcorpus( )함수에 brwon_single 데이터 폴더를 인자로 넣어, 그 결과를 brown_corp변수에 할당한다.\n",
        "-- [코드문법3] [단어 토큰화] ct모듈의 tokenize( )함수 인자로 바로 위 코드 변수 brown_corp를 넣어, 그 결과를 tok_corp변수에 할당한다.\n",
        "-- [코드문법4] [토큰화한 단어 빈도수] ct모듈의 frequency( )함수 인자로 바로 위 코드 변수 tok_corp를 넣어, 그 결과를 brown_freq변수에 할당한다.\n",
        "-- [코드문법5] ct모듈의 head( )에 brown_freq변수를 첫 째 인자로, 빈도수가 높은 것 10개를 둘 째 인자로 사용해서, 상위 10개 토큰의 빈도수를 출력한다."
      ],
      "metadata": {
        "id": "bO38yLp90864"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab 작업 디렉토리에서 폴더를 생성하여 다른 곳에서 다운로드 받은 단/복수의 파일을 옮겨 넣는 방법 \n",
        "- 코랩에 기본으로 깔려 있는 sample_data 에 커서를 대면 활성화되면서 우측에 세로 점 3 개가 나타난다. 거기를 클릭하면 선택사항 -> new folder를 클릭한다. \n",
        "- new folder를 drag해서 sample_data 쪽으로 끌어오면 같은 레벨로 폴더가 이동한다. \n",
        "- new folder에 커서를 대면 활성화되면서 우측에 세로 점 3 개가 나타난다.Rename folder 클릭해서 파일명을 새이름으로 바꾼다. 예)brown_single\n",
        "- 해당 폴더에 커서를 대면 활성화되면서 우측에 세로 점 3 개가 나타난다.Upload 클릭해서 본인 컴퓨터에 다운로드 받은 복수의 파일을 모두 업로드 한다. "
      ],
      "metadata": {
        "id": "IpAwLbK32qeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from corpus_toolkit import corpus_tools as ct\n",
        "brown_corp = ct.ldcorpus(\"brown_single\") #load and read corpus: ca~cd from brown_single original folder for class use\n",
        "tok_corp = ct.tokenize(brown_corp) #tokenize corpus - by default this lemmatizes as well\n",
        "brown_freq = ct.frequency(tok_corp) #creates a frequency dictionary\n",
        "##note that range can be calculated instead of frequency using the argument calc = \"range\"\n",
        "ct.head(brown_freq, hits = 10) #print top 10 items"
      ],
      "metadata": {
        "id": "17lsll3VIzu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 명령어 차곡히 쌓기 (Nest) \n",
        "위에서 파일을 로드해서 읽고 -> 토큰화 하고 -> 토큰의 빈도수를 계산하는 명령어를 사용하여 코드로 작성했다. 아래 코드와 같이 모듈.함수( )인 명령어를 차곡차곡 쌓아서 같은 목적을 달성할 수 있고 이를 추천한다. "
      ],
      "metadata": {
        "id": "R-98Zbi6j8qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown_freq = ct.frequency(ct.tokenize(ct.ldcorpus(\"brown_single\")))\n",
        "ct.head(brown_freq, hits = 10)"
      ],
      "metadata": {
        "id": "zxrWoaKbj-Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results1 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"],nhits = 10)\n",
        "for x in conc_results1:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "id": "_z3M-DivngQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results2 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"],collocates = [\"suddenly\", 'just'], nhits = 10)\n",
        "for x in conc_results2:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "id": "AZRzHbwUnjgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collocates = ct.collocator(ct.tokenize(ct.ldcorpus(\"brown_single\")),\"go\",stat = \"MI\")\n",
        "#stat options include: \"MI\", \"T\", \"freq\", \"left\", and \"right\"\n",
        "\n",
        "ct.head(collocates, hits = 10)"
      ],
      "metadata": {
        "id": "TWCovRiTtJtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##N-grams\n",
        "N-grams are contiguous sequences of n words. The tokenize() function can be used to create an n-gram version of a corpus by employing the ngram argument. By default, words in an n-gram are separated by two underscores \"__\""
      ],
      "metadata": {
        "id": "AC3sbfm_uRO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigramfreq = ct.frequency(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False, ngram = 3))\n",
        "ct.head(trigramfreq, hits = 10)"
      ],
      "metadata": {
        "id": "Ll9zKBO4uAo0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}