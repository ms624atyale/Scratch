{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM/+P85SpP073WB6m6IBTYd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms624atyale/Scratch/blob/main/Text_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ¹ ğŸ¾ [Text Corpus <font size='1.8'>ì½”í¼ìŠ¤/ë§ë­‰ì¹˜</font>](https://en.wikipedia.org/wiki/Text_corpus)  \n",
        "- In linguistics, a corpus (plural corpora) or text corpus is a language resource consisting of a large and structured set of texts (nowadays usually electronically stored and processed). In corpus linguistics, they are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.\n",
        "\n",
        "- The **corpus-toolkit** package grew out of courses in corpus linguistics and learner corpus research. The toolkit attempts to balance simplicity of use, broad application, and scalability. Common corpus analyses such as the <font color = 'pink'>_calculation of word and n-gram frequency and range, keyness, and collocation_</font> are included. In addition, more advanced analyses such as the identification of <font color = 'pink'>_dependency bigrams (e.g., verb-direct object combinations) and their frequency, range, and strength of association_</font>  are also included.(https://pypi.org/project/corpus-toolkit/)\n",
        "\n",
        "Some conditions should be fulfilled if you want to conduct corpus-related analysis. \n",
        "\n",
        ">1. Read and write a file using an operating system package.\n",
        "\n",
        "ğŸ†˜ Install the **[os](https://docs.python.org/3/library/os.html)** package.\n",
        "\n",
        ">2. Text files you want to analyze (e.g., url(uniform resource locator) with html document, text files under the Files dicrectory of Google Colab).\n",
        ">3. Text â¡ï¸ Words: **Tokenization**\n",
        ">4. Words with the conjugation, inflection, derivation process â†”ï¸ Words sorted by grouping inflected or variant forms of the same word (i.e., **lemmatization**)\n",
        ">5. POS (part of speech (e.g., word-grammatical category pairs))\n",
        "\n",
        "\n",
        "ğŸ†˜ Install **corpus-toolkit** and **nltk**(natural language tool kit) packages.\n",
        " \n"
      ],
      "metadata": {
        "id": "M7afPLwh-bRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ“Œ Download the os package \n",
        "#@markdown ğŸ“ Open a txt file you want to analyze.\n",
        "import os\n",
        "#?os.mkdir(\"txtdata\")"
      ],
      "metadata": {
        "id": "J-ZgsoO2LnYg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBz4z96Z-YV5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ğŸ“Œ Download the corpus-toolkit package\n",
        "!pip install corpus-toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-processing (ì‚¬ì „ì²˜ë¦¬)\n",
        "- ì½”í¼ìŠ¤ ë¶„ì„ì„ í•˜ë ¤ë©´ ë¶„ì„í•˜ë ¤ëŠ” í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ë¥¼ í† í°í™”í•˜ì—¬ ë¶„ì„í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ì¤€ë¹„í•´ ë†“ì•„ì•¼ í•œë‹¤. i) tokenize í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°ì„ ë¦¬ìŠ¤íŠ¸í™” í•˜ì—¬ ì¤€ë¹„í•´ ë†“ì„ ìˆ˜ë„ ìˆê³ , ii) tag í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°ì— ë¬¸ë²•ë²”ì£¼(í’ˆì‚¬)ë¥¼ ì—°ê²°í•´ì„œ ì¤€ë¹„í•´ ë†“ì„ ìˆ˜ë„ ìˆë‹¤.  \n",
        "-- [ì½”ë“œë¬¸ë²•1] corpus_toolkitíŒ¨í‚¤ì§€ì—ì„œ corpus_toolsëª¨ë“ˆì„ ctë¡œ ì¤„ì—¬ì„œ ë¶ˆëŸ¬ë“¤ì—¬ë¼.\n",
        "-- [ì½”ë“œë¬¸ë²•2] [íŒŒì¼ ì˜¬ë¦¬ê³  ì½ê¸°] ctëª¨ë“ˆì˜ ldcorpus( )í•¨ìˆ˜ì— brwon_single ë°ì´í„° í´ë”ë¥¼ ì¸ìë¡œ ë„£ì–´, ê·¸ ê²°ê³¼ë¥¼ brown_corpë³€ìˆ˜ì— í• ë‹¹í•œë‹¤.\n",
        "-- [ì½”ë“œë¬¸ë²•3] [ë‹¨ì–´ í† í°í™”] ctëª¨ë“ˆì˜ tokenize( )í•¨ìˆ˜ ì¸ìë¡œ ë°”ë¡œ ìœ„ ì½”ë“œ ë³€ìˆ˜ brown_corpë¥¼ ë„£ì–´, ê·¸ ê²°ê³¼ë¥¼ tok_corpë³€ìˆ˜ì— í• ë‹¹í•œë‹¤.\n",
        "-- [ì½”ë“œë¬¸ë²•4] [í† í°í™”í•œ ë‹¨ì–´ ë¹ˆë„ìˆ˜] ctëª¨ë“ˆì˜ frequency( )í•¨ìˆ˜ ì¸ìë¡œ ë°”ë¡œ ìœ„ ì½”ë“œ ë³€ìˆ˜ tok_corpë¥¼ ë„£ì–´, ê·¸ ê²°ê³¼ë¥¼ brown_freqë³€ìˆ˜ì— í• ë‹¹í•œë‹¤.\n",
        "-- [ì½”ë“œë¬¸ë²•5] ctëª¨ë“ˆì˜ head( )ì— brown_freqë³€ìˆ˜ë¥¼ ì²« ì§¸ ì¸ìë¡œ, ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ê²ƒ 10ê°œë¥¼ ë‘˜ ì§¸ ì¸ìë¡œ ì‚¬ìš©í•´ì„œ, ìƒìœ„ 10ê°œ í† í°ì˜ ë¹ˆë„ìˆ˜ë¥¼ ì¶œë ¥í•œë‹¤."
      ],
      "metadata": {
        "id": "bO38yLp90864"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab ì‘ì—… ë””ë ‰í† ë¦¬ì—ì„œ í´ë”ë¥¼ ìƒì„±í•˜ì—¬ ë‹¤ë¥¸ ê³³ì—ì„œ ë‹¤ìš´ë¡œë“œ ë°›ì€ ë‹¨/ë³µìˆ˜ì˜ íŒŒì¼ì„ ì˜®ê²¨ ë„£ëŠ” ë°©ë²• \n",
        "- ì½”ë©ì— ê¸°ë³¸ìœ¼ë¡œ ê¹”ë ¤ ìˆëŠ” sample_data ì— ì»¤ì„œë¥¼ ëŒ€ë©´ í™œì„±í™”ë˜ë©´ì„œ ìš°ì¸¡ì— ì„¸ë¡œ ì  3 ê°œê°€ ë‚˜íƒ€ë‚œë‹¤. ê±°ê¸°ë¥¼ í´ë¦­í•˜ë©´ ì„ íƒì‚¬í•­ -> new folderë¥¼ í´ë¦­í•œë‹¤. \n",
        "- new folderë¥¼ dragí•´ì„œ sample_data ìª½ìœ¼ë¡œ ëŒì–´ì˜¤ë©´ ê°™ì€ ë ˆë²¨ë¡œ í´ë”ê°€ ì´ë™í•œë‹¤. \n",
        "- new folderì— ì»¤ì„œë¥¼ ëŒ€ë©´ í™œì„±í™”ë˜ë©´ì„œ ìš°ì¸¡ì— ì„¸ë¡œ ì  3 ê°œê°€ ë‚˜íƒ€ë‚œë‹¤.Rename folder í´ë¦­í•´ì„œ íŒŒì¼ëª…ì„ ìƒˆì´ë¦„ìœ¼ë¡œ ë°”ê¾¼ë‹¤. ì˜ˆ)brown_single\n",
        "- í•´ë‹¹ í´ë”ì— ì»¤ì„œë¥¼ ëŒ€ë©´ í™œì„±í™”ë˜ë©´ì„œ ìš°ì¸¡ì— ì„¸ë¡œ ì  3 ê°œê°€ ë‚˜íƒ€ë‚œë‹¤.Upload í´ë¦­í•´ì„œ ë³¸ì¸ ì»´í“¨í„°ì— ë‹¤ìš´ë¡œë“œ ë°›ì€ ë³µìˆ˜ì˜ íŒŒì¼ì„ ëª¨ë‘ ì—…ë¡œë“œ í•œë‹¤. "
      ],
      "metadata": {
        "id": "IpAwLbK32qeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from corpus_toolkit import corpus_tools as ct\n",
        "brown_corp = ct.ldcorpus(\"brown_single\") #load and read corpus: ca~cd from brown_single original folder for class use\n",
        "tok_corp = ct.tokenize(brown_corp) #tokenize corpus - by default this lemmatizes as well\n",
        "brown_freq = ct.frequency(tok_corp) #creates a frequency dictionary\n",
        "##note that range can be calculated instead of frequency using the argument calc = \"range\"\n",
        "ct.head(brown_freq, hits = 10) #print top 10 items"
      ],
      "metadata": {
        "id": "17lsll3VIzu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ëª…ë ¹ì–´ ì°¨ê³¡íˆ ìŒ“ê¸° (Nest) \n",
        "ìœ„ì—ì„œ íŒŒì¼ì„ ë¡œë“œí•´ì„œ ì½ê³  -> í† í°í™” í•˜ê³  -> í† í°ì˜ ë¹ˆë„ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì½”ë“œë¡œ ì‘ì„±í–ˆë‹¤. ì•„ë˜ ì½”ë“œì™€ ê°™ì´ ëª¨ë“ˆ.í•¨ìˆ˜( )ì¸ ëª…ë ¹ì–´ë¥¼ ì°¨ê³¡ì°¨ê³¡ ìŒ“ì•„ì„œ ê°™ì€ ëª©ì ì„ ë‹¬ì„±í•  ìˆ˜ ìˆê³  ì´ë¥¼ ì¶”ì²œí•œë‹¤. "
      ],
      "metadata": {
        "id": "R-98Zbi6j8qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown_freq = ct.frequency(ct.tokenize(ct.ldcorpus(\"brown_single\")))\n",
        "ct.head(brown_freq, hits = 10)"
      ],
      "metadata": {
        "id": "zxrWoaKbj-Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results1 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"],nhits = 10)\n",
        "for x in conc_results1:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "id": "_z3M-DivngQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results2 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"],collocates = [\"suddenly\", 'just'], nhits = 10)\n",
        "for x in conc_results2:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "id": "AZRzHbwUnjgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collocates = ct.collocator(ct.tokenize(ct.ldcorpus(\"brown_single\")),\"go\",stat = \"MI\")\n",
        "#stat options include: \"MI\", \"T\", \"freq\", \"left\", and \"right\"\n",
        "\n",
        "ct.head(collocates, hits = 10)"
      ],
      "metadata": {
        "id": "TWCovRiTtJtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##N-grams\n",
        "N-grams are contiguous sequences of n words. The tokenize() function can be used to create an n-gram version of a corpus by employing the ngram argument. By default, words in an n-gram are separated by two underscores \"__\""
      ],
      "metadata": {
        "id": "AC3sbfm_uRO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigramfreq = ct.frequency(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False, ngram = 3))\n",
        "ct.head(trigramfreq, hits = 10)"
      ],
      "metadata": {
        "id": "Ll9zKBO4uAo0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}