{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordCloud_ModifiedfromMK316.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNkK5LgFlxdp+JGqjyOs2UX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms624atyale/Scratch/blob/main/WordCloud_ModifiedfromMK316.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üêπ üêæ [Text Corpus <font size='1.8'>ÏΩîÌçºÏä§/ÎßêÎ≠âÏπò</font>](https://en.wikipedia.org/wiki/Text_corpus)  \n",
        "- In linguistics, a corpus (plural corpora) or text corpus is a language resource consisting of a large and structured set of texts (nowadays usually electronically stored and processed). In corpus linguistics, they are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.\n",
        "\n",
        "- The **corpus-toolkit** package grew out of courses in corpus linguistics and learner corpus research. The toolkit attempts to balance simplicity of use, broad application, and scalability. Common corpus analyses such as the <font color = 'pink'>_calculation of word and n-gram frequency and range, keyness, and collocation_</font> are included. In addition, more advanced analyses such as the identification of <font color = 'pink'>_dependency bigrams (e.g., verb-direct object combinations) and their frequency, range, and strength of association_</font>  are also included.(https://pypi.org/project/corpus-toolkit/)\n",
        "\n",
        "Some conditions should be fulfilled if you want to conduct corpus-related analysis. \n",
        "\n",
        ">1. Read and write a file using an operating system package.\n",
        ">2. üÜò import the **[os](https://docs.python.org/3/library/os.html)** module.\n",
        "\n",
        "\n",
        ">3. Text files you want to analyze (e.g., url(uniform resource locator) with html document, text files under the Files dicrectory of Google Colab).\n",
        ">4. Text ‚û°Ô∏è Words: **Tokenization**\n",
        ">5. Words with the conjugation, inflection, derivation process ‚ÜîÔ∏è Words sorted by grouping inflected or variant forms of the same word (i.e., **lemmatization**)\n",
        ">6. POS (part of speech (e.g., word-grammatical category pairs))\n",
        ">7. üÜò Install **corpus-toolkit** and **nltk**(natural language tool kit) packages.\n",
        " \n"
      ],
      "metadata": {
        "id": "M7afPLwh-bRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Download the os module \n",
        "import os"
      ],
      "metadata": {
        "cellView": "form",
        "id": "J-ZgsoO2LnYg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Make a new working directory as \"txtdata\". üìé <Module name: os> <function: mkdir>\n",
        "\n",
        "os.mkdir(\"txtdata\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Nt17SlKpUq_f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBz4z96Z-YV5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown üìå Download the corpus-toolkit package\n",
        "!pip install corpus-toolkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Get working directory. <codeline: print working directory>\n",
        "%pwd"
      ],
      "metadata": {
        "id": "6YiwfB3fVfQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Open a txt file. <Use a set of duble quotation marks \"\" and assign the url address as the _url_ variable> \n",
        "url=\"https://github.com/ms624atyale/Data_Misc/blob/main/crime_punishment.txt\"\n",
        "\n",
        "os.system(\"curl \" + url + \" > txtdata.txt\") #This moves the whole text of the url to the txtdata folder.  \n",
        "\n",
        "file = open(\"txtdata.txt\")\n",
        "text = file.read().replace(\"\\n\", \" \") #Replace line with a space.\n",
        "file.close() #Close the file you have been working on.\n",
        "\n",
        "#@markdown üìé When you see txtdata.txt under the Files directory, move it under the txtdata folder you've created by drag & drop."
      ],
      "metadata": {
        "id": "cn5UjS8xWUpN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå i) Tokenize your text and ii) Get frequency.\n",
        "\n",
        "from corpus_toolkit import corpus_tools as ct\n",
        "mydata = ct.ldcorpus(\"txtdata\") #load and read text files under 'txtdata' directory\n",
        "tok_corp = ct.tokenize(mydata) #tokenize corpus - by default this lemmatizes as well\n",
        "mydata = ct.frequency(mydata) #creates a frequency dictionary"
      ],
      "metadata": {
        "id": "u-Ph5SUx79UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Tagging (i.e., associating each token with a grammatical category (e.g., mountain - N) )\n",
        "ct.write_corpus(\"tagged_txt\",ct.tag(ct.ldcorpus(\"txtdata\")))"
      ],
      "metadata": {
        "id": "EZ7XatV0ZpMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Get frequency of your tagged tokens. 'hits=10' means you want to get the top 10 words. \n",
        "\n",
        "tagged_freq = ct.frequency(ct.reload(\"tagged_txt\"))\n",
        "ct.head(tagged_freq, hits = 10)"
      ],
      "metadata": {
        "id": "TxTrpzkEaXiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí° Now, let's save tagged data as a dataframe and get word clouds!"
      ],
      "metadata": {
        "id": "1DoJZCxobIrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå  Tagged data is in a dictionary format (e.g., {key:value}).\n",
        "type(tagged_freq)"
      ],
      "metadata": {
        "id": "YDjwNH2xbh6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Import the pandas package so as to handle dataframe.\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "OvJt1jQBb6KD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = tagged_freq\n",
        "data_items = data_dict.items()\n",
        "data_list = list(data_items)\n",
        "df = pd.DataFrame(data_list)\n",
        "df.columns = [\"Tagged\",\"Freq\"]\n",
        "print(df)"
      ],
      "metadata": {
        "id": "bpGw92j8cRx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí° Splitting tagged columns into Words and POS <font size = '2.3'> part of speech (i.e., grammatical categories)\n",
        "  - e.g., \n",
        "              column          column 1.    column2\n",
        "          yesterday_ADP ‚û°Ô∏è   yesterday       ADP\n",
        "          rain_NOUN             rain         NOUN\n",
        "          yellow_ADJ           yellow.       ADJ"
      ],
      "metadata": {
        "id": "riajmUc-cfJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged = df[\"Tagged\"]\n",
        "pos = []\n",
        "word = []\n",
        "\n",
        "for i in range(0, len(tagged)):\n",
        "  w = tagged[i]\n",
        "  ws = w.split(\"_\")\n",
        "  word.append(ws[0])\n",
        "  pos.append(ws[1])\n",
        "\n",
        "print(len(tagged))\n",
        "print(word[:10])\n",
        "print(pos[:10])"
      ],
      "metadata": {
        "id": "_Is6HP1udiNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Add new columns to the dataframe.\n",
        "\n",
        "df[\"POS\"] = pos\n",
        "df[\"Word\"] = word\n",
        "\n",
        "# Rearranging column order (remove Tagged column)\n",
        "cols = [\"POS\",\"Word\",\"Freq\"]\n",
        "df = df[cols]\n",
        "\n",
        "# Sort by POS and Freq\n",
        "df = df.sort_values(by=['POS', 'Freq'], ascending = False)\n",
        "print(\"Total rows: \", len(df))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "xVE9PFHvfBdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚õÖ  Creating wordclouds by POS\n",
        ">1. Below, \"wc\" takes text, not list. So we make word-list-by-POS into a text using 'join'.\n",
        ">2. In addition, the joined text should include words according to their frequency. (e.g., if \"before\" occurs 5 times, then the text should include \"before before before before before\"."
      ],
      "metadata": {
        "id": "G-UzLDU7fdSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Select POS = VERB\n",
        "df1 = df[df[\"POS\"] == \"VERB\"]; print(len(df1))\n",
        "\n",
        "# as list\n",
        "freq1 = list(df1[\"Freq\"]); print(len(freq1))\n",
        "txt1 = list(df1[\"Word\"]); print(len(txt1))\n",
        "\n",
        "# Repeat words by Freq\n",
        "import numpy as np\n",
        "# x = np.array(txt1)\n",
        "y = np.repeat(txt1, freq1, axis=0)\n",
        "y = list(y)\n",
        "\n",
        "txt2 = ' '.join(y); print(len(txt2))\n",
        "print(txt2)"
      ],
      "metadata": {
        "id": "AsIqOSfdgD5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå High frequency POS\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wc = WordCloud().generate(str(txt2)) \n",
        "plt.imshow(wc)\n"
      ],
      "metadata": {
        "id": "7s4NgSEFgKRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚õÖ ‚õÖThe following codes will provide you POS options such as NOUN, ADJ, ADV. \n",
        "\n",
        "## üìé Select POS you want. "
      ],
      "metadata": {
        "id": "2D5xOgIXggbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Wordcloud by POS:\n",
        "\n",
        "pos = \"ADJ\" #@param = [\"VERB\",\"NOUN\",\"ADJ\",\"ADV\"]\n",
        "df1 = df[df[\"POS\"] == pos]; print(len(df1))\n",
        "\n",
        "# as list\n",
        "freq1 = list(df1[\"Freq\"])\n",
        "txt1 = list(df1[\"Word\"])\n",
        "\n",
        "# Repeat words by Freq\n",
        "import numpy as np\n",
        "# x = np.array(txt1)\n",
        "y = np.repeat(txt1, freq1, axis=0)\n",
        "y = list(y)\n",
        "\n",
        "txt2 = ' '.join(y)\n",
        "\n",
        "print(df[df[\"POS\"] == pos])\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Collocations = False (otherwise, Wordcloud takes 'light light' as a collocation and add it on the wordcloud )\n",
        "wc = WordCloud(collocations = False).generate(str(txt2)) \n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "siL_Cw3Dg2tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚õÖ ‚õÖ I don't like the background in black. I want it WHITE!!! ‚õÑ‚ö°"
      ],
      "metadata": {
        "id": "DOfE0T5GhgmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "#stopwords.add('us')\n",
        "len(stopwords)\n",
        "spltxt = text.split()"
      ],
      "metadata": {
        "id": "uj2LcGCgiNf8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Wordcloud by POS:\n",
        "\n",
        "pos = \"VERB\" #@param = [\"VERB\",\"NOUN\",\"ADJ\",\"ADV\"]\n",
        "df1 = df[df[\"POS\"] == pos]; print(len(df1))\n",
        "\n",
        "# as list\n",
        "freq1 = list(df1[\"Freq\"])\n",
        "txt1 = list(df1[\"Word\"])\n",
        "\n",
        "# Repeat words by Freq\n",
        "import numpy as np\n",
        "# x = np.array(txt1)\n",
        "y = np.repeat(txt1, freq1, axis=0)\n",
        "spltxt = list(y)\n",
        "\n",
        "wordcloud = WordCloud(stopwords=stopwords,relative_scaling = 0.2, random_state=3, collocations = False,\n",
        "                      max_words=2000, background_color='white').generate(' '.join(spltxt))\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "#plt.show()\n",
        "#plt.savefig('wordcloud_title.png')\n",
        "wordcloud.to_file('wordcloud_title.png')\n",
        "# wordcloud.to_file('docs/png/wordcloud_title.png')"
      ],
      "metadata": {
        "id": "iFwwcp4YiThg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}