{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LexicalAnalysis_ConcordanceCollocation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM8FNDyptD2Pcbd6GzyzXHJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms624atyale/Scratch/blob/main/LexicalAnalysis_ConcordanceCollocation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üò±üí¶ Before you want to analyze text corpus, <font size = '3.0'>you should do **steps essentials** for further text analysis such as concordance and collocation. \n",
        "\n",
        "#‚è¨ ‚¨áÔ∏è ‚è¨ ‚¨áÔ∏è ‚è¨ ‚¨áÔ∏è ‚è¨ ‚¨áÔ∏è ‚è¨ ‚¨áÔ∏è ‚è¨ ‚¨áÔ∏è ‚è¨ ‚¨áÔ∏è ‚è¨ ‚¨áÔ∏è ‚è¨ ‚¨áÔ∏è ‚è¨ ‚¨áÔ∏è  \n",
        "\n",
        "#üêπ üêæ Essential Steps for Text Analysis\n",
        "\n",
        "## üìö üëÄ [Text Corpus <font size='1.8'>ÏΩîÌçºÏä§/ÎßêÎ≠âÏπò</font>](https://en.wikipedia.org/wiki/Text_corpus)  \n",
        "- In linguistics, a corpus (plural corpora) or text corpus is a language resource consisting of a large and structured set of texts (nowadays usually electronically stored and processed). In corpus linguistics, they are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.\n",
        "\n",
        "\n",
        "## üìö üëÄ Text analysis all starts with tokenization!\n",
        "Before you conduct corpus-related analysis, all words in your text should be **tokenized** (i.e., break text into individual units (e.g., words)). In particular, words with conjugation, derivation, or inflection are tokenized based on its base forms (e.g., stem). Tokenized words can be further associated with thier grammatical categories (e.g., NOUN, VERB, ADJ, etc.), which we call it **POS (part of speech)**. Pairwise units (e.g., word_POS) can employ a **dictionary** form where a key is paired with its value.  \n",
        "\n",
        "- 1. Read and write a file using an operating system package.\n",
        "    - üÜò import the **[os](https://docs.python.org/3/library/os.html)** module.\n",
        "- 2. Analyze text corpus.\n",
        "    - üÜò Install **corpus-toolkit** \n",
        "- 3. Aanlyze natural language.\n",
        "    - üÜò Install **nltk**(i.e., natural language tool kit) packages.\n",
        "    - üÜò Import **re** (i.e., regular expression module in Python)\n",
        "- 4. Arrange your data into a structured data frame.\n",
        "    - üÜò Install **pandas**\n",
        "\n",
        "## üìö üëÄ Don't miss this!\n",
        ">1. Whatever operating system you use, your computer has a directory which contains folders, files, and subdirectories in a hierarchical manner. Python also has an [os module](https://docs.python.org/3/library/os.html), and you can **read** and **write** files after you import it.\n",
        ">2. Text files can be ready for use as follows. \n",
        "  - text=\"url(uniform resource locator) with html document\" \n",
        "    - <font size='2.0'> https://raw.githubusercontent.com/ms624atyale/Data_Misc/main/TheAesop_theFoxwithoutaTail.txt</font>\n",
        "  - text files under the Files dicrectory of Google Colab.\n",
        "  - Use a codeline on a Code cell\n",
        "    txt = \"\"\"_copy and paste a text of your interest from a url of websites or html (HyperTextMarkup Language)_\"\"\" \n",
        " \n"
      ],
      "metadata": {
        "id": "M7afPLwh-bRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Download the os module \n",
        "import os"
      ],
      "metadata": {
        "id": "J-ZgsoO2LnYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Make a new working directory as \"txtdata\". üìé <Module name: os> <function: mkdir>\n",
        "\n",
        "os.mkdir(\"txtfolder\")"
      ],
      "metadata": {
        "id": "Nt17SlKpUq_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JBz4z96Z-YV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff798d41-b9da-4bbb-8e09-6671d8307086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting corpus-toolkit\n",
            "  Downloading corpus_toolkit-0.32-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.7 MB 14.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: corpus-toolkit\n",
            "Successfully installed corpus-toolkit-0.32\n"
          ]
        }
      ],
      "source": [
        "#@markdown üìå Download the corpus-toolkit package\n",
        "!pip install corpus-toolkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lexical-diversity\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bWvAY-gxSf9",
        "outputId": "71345800-0a4e-4f24-bd2d-da681328810d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lexical-diversity\n",
            "  Downloading lexical_diversity-0.1.1-py3-none-any.whl (117 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117 kB 12.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: lexical-diversity\n",
            "Successfully installed lexical-diversity-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lexical_diversity import lex_div as ld"
      ],
      "metadata": {
        "id": "dLY2Eo6NugTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "cHNMVuCExYrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Get working directory. <code line: print working directory>\n",
        "%pwd"
      ],
      "metadata": {
        "id": "6YiwfB3fVfQy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5ba1115a-b6a9-4164-9546-810b224a7730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Make a folder (e.g., brown_single_folder)under the FILE directory of Colab, and upload files from your machine.\n",
        "\n",
        "from corpus_toolkit import corpus_tools as ct\n",
        "\n",
        "brown_corp = ct.ldcorpus(\"brown_single_folder\") #load and read corpus: ca~cd from brown_single original folder for class use"
      ],
      "metadata": {
        "id": "y7_FU_bCx_UN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå Text ‚û°Ô∏è Words: **Tokenization**\n",
        "\n",
        "üìé  Words sorted by grouping inflected or variant forms of the same word (i.e., **lemmatization**) ‚ÜîÔ∏è Words with the conjugation, inflection, derivation process"
      ],
      "metadata": {
        "id": "LL3q_o_b7DcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Lemmatization\n",
        "tok_corp = ct.tokenize(brown_corp) #tokenize corpus - by default this lemmatizes as well"
      ],
      "metadata": {
        "id": "KP-EzJHeuxwM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Word Frequency\n",
        "brown_freq = ct.frequency(tok_corp) #creates a frequency dictionary\n",
        "##note that range can be calculated instead of frequency using the argument calc = \"range\"\n",
        "ct.head(brown_freq, hits = 10) #print top 10 items"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T3P_QvC-FUj",
        "outputId": "f65da464-c113-47a9-8be8-291f9cf02306"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ca_ca16.txt (1 of 88 files)\n",
            "Processing cb_cb25.txt (2 of 88 files)\n",
            "Processing cc_cc03.txt (3 of 88 files)\n",
            "Processing cc_cc16.txt (4 of 88 files)\n",
            "Processing cb_cb04.txt (5 of 88 files)\n",
            "Processing cc_cc05.txt (6 of 88 files)\n",
            "Processing ca_ca39.txt (7 of 88 files)\n",
            "Processing cb_cb03.txt (8 of 88 files)\n",
            "Processing ca_ca13.txt (9 of 88 files)\n",
            "Processing ca_ca19.txt (10 of 88 files)\n",
            "Processing cb_cb20.txt (11 of 88 files)\n",
            "Processing cc_cc06.txt (12 of 88 files)\n",
            "Processing cc_cc17.txt (13 of 88 files)\n",
            "Processing cc_cc09.txt (14 of 88 files)\n",
            "Processing ca_ca11.txt (15 of 88 files)\n",
            "Processing ca_ca30.txt (16 of 88 files)\n",
            "Processing ca_ca02.txt (17 of 88 files)\n",
            "Processing cb_cb06.txt (18 of 88 files)\n",
            "Processing cc_cc12.txt (19 of 88 files)\n",
            "Processing cb_cb18.txt (20 of 88 files)\n",
            "Processing cb_cb17.txt (21 of 88 files)\n",
            "Processing ca_ca33.txt (22 of 88 files)\n",
            "Processing ca_ca27.txt (23 of 88 files)\n",
            "Processing ca_ca44.txt (24 of 88 files)\n",
            "Processing ca_ca21.txt (25 of 88 files)\n",
            "Processing ca_ca20.txt (26 of 88 files)\n",
            "Processing ca_ca37.txt (27 of 88 files)\n",
            "Processing cb_cb09.txt (28 of 88 files)\n",
            "Processing cc_cc14.txt (29 of 88 files)\n",
            "Processing ca_ca25.txt (30 of 88 files)\n",
            "Processing ca_ca24.txt (31 of 88 files)\n",
            "Processing ca_ca22.txt (32 of 88 files)\n",
            "Processing ca_ca18.txt (33 of 88 files)\n",
            "Processing cc_cc08.txt (34 of 88 files)\n",
            "Processing ca_ca40.txt (35 of 88 files)\n",
            "Processing ca_ca07.txt (36 of 88 files)\n",
            "Processing ca_ca31.txt (37 of 88 files)\n",
            "Processing cb_cb22.txt (38 of 88 files)\n",
            "Processing ca_ca12.txt (39 of 88 files)\n",
            "Processing cc_cc10.txt (40 of 88 files)\n",
            "Processing ca_ca17.txt (41 of 88 files)\n",
            "Processing ca_ca35.txt (42 of 88 files)\n",
            "Processing ca_ca06.txt (43 of 88 files)\n",
            "Processing ca_ca29.txt (44 of 88 files)\n",
            "Processing ca_ca23.txt (45 of 88 files)\n",
            "Processing ca_ca28.txt (46 of 88 files)\n",
            "Processing cb_cb11.txt (47 of 88 files)\n",
            "Processing cc_cc11.txt (48 of 88 files)\n",
            "Processing ca_ca42.txt (49 of 88 files)\n",
            "Processing cb_cb02.txt (50 of 88 files)\n",
            "Processing cb_cb21.txt (51 of 88 files)\n",
            "Processing cb_cb10.txt (52 of 88 files)\n",
            "Processing cb_cb16.txt (53 of 88 files)\n",
            "Processing ca_ca04.txt (54 of 88 files)\n",
            "Processing cc_cc15.txt (55 of 88 files)\n",
            "Processing ca_ca10.txt (56 of 88 files)\n",
            "Processing ca_ca38.txt (57 of 88 files)\n",
            "Processing cb_cb24.txt (58 of 88 files)\n",
            "Processing cb_cb14.txt (59 of 88 files)\n",
            "Processing ca_ca41.txt (60 of 88 files)\n",
            "Processing cb_cb05.txt (61 of 88 files)\n",
            "Processing cc_cc13.txt (62 of 88 files)\n",
            "Processing cc_cc04.txt (63 of 88 files)\n",
            "Processing ca_ca36.txt (64 of 88 files)\n",
            "Processing cc_cc01.txt (65 of 88 files)\n",
            "Processing ca_ca43.txt (66 of 88 files)\n",
            "Processing cc_cc07.txt (67 of 88 files)\n",
            "Processing cb_cb13.txt (68 of 88 files)\n",
            "Processing ca_ca26.txt (69 of 88 files)\n",
            "Processing cb_cb12.txt (70 of 88 files)\n",
            "Processing cb_cb15.txt (71 of 88 files)\n",
            "Processing cb_cb01.txt (72 of 88 files)\n",
            "Processing cb_cb27.txt (73 of 88 files)\n",
            "Processing ca_ca05.txt (74 of 88 files)\n",
            "Processing ca_ca32.txt (75 of 88 files)\n",
            "Processing ca_ca09.txt (76 of 88 files)\n",
            "Processing cb_cb19.txt (77 of 88 files)\n",
            "Processing ca_ca08.txt (78 of 88 files)\n",
            "Processing ca_ca14.txt (79 of 88 files)\n",
            "Processing cb_cb26.txt (80 of 88 files)\n",
            "Processing cb_cb23.txt (81 of 88 files)\n",
            "Processing ca_ca03.txt (82 of 88 files)\n",
            "Processing cb_cb08.txt (83 of 88 files)\n",
            "Processing ca_ca34.txt (84 of 88 files)\n",
            "Processing cc_cc02.txt (85 of 88 files)\n",
            "Processing ca_ca01.txt (86 of 88 files)\n",
            "Processing ca_ca15.txt (87 of 88 files)\n",
            "Processing cb_cb07.txt (88 of 88 files)\n",
            "the\t12711\n",
            "be\t6286\n",
            "of\t6191\n",
            "a\t5446\n",
            "and\t4701\n",
            "to\t4453\n",
            "in\t3837\n",
            "he\t2443\n",
            "have\t1953\n",
            "for\t1810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brown_freq = ct.frequency(ct.tokenize(ct.ldcorpus(\"brown_single_folder\")))\n",
        "ct.head(brown_freq, hits = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0WWMbzXWiZ0",
        "outputId": "e3550c3f-09ba-4b6b-ae99-9166c9b76b11"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ca_ca16.txt (1 of 88 files)\n",
            "Processing cb_cb25.txt (2 of 88 files)\n",
            "Processing cc_cc03.txt (3 of 88 files)\n",
            "Processing cc_cc16.txt (4 of 88 files)\n",
            "Processing cb_cb04.txt (5 of 88 files)\n",
            "Processing cc_cc05.txt (6 of 88 files)\n",
            "Processing ca_ca39.txt (7 of 88 files)\n",
            "Processing cb_cb03.txt (8 of 88 files)\n",
            "Processing ca_ca13.txt (9 of 88 files)\n",
            "Processing ca_ca19.txt (10 of 88 files)\n",
            "Processing cb_cb20.txt (11 of 88 files)\n",
            "Processing cc_cc06.txt (12 of 88 files)\n",
            "Processing cc_cc17.txt (13 of 88 files)\n",
            "Processing cc_cc09.txt (14 of 88 files)\n",
            "Processing ca_ca11.txt (15 of 88 files)\n",
            "Processing ca_ca30.txt (16 of 88 files)\n",
            "Processing ca_ca02.txt (17 of 88 files)\n",
            "Processing cb_cb06.txt (18 of 88 files)\n",
            "Processing cc_cc12.txt (19 of 88 files)\n",
            "Processing cb_cb18.txt (20 of 88 files)\n",
            "Processing cb_cb17.txt (21 of 88 files)\n",
            "Processing ca_ca33.txt (22 of 88 files)\n",
            "Processing ca_ca27.txt (23 of 88 files)\n",
            "Processing ca_ca44.txt (24 of 88 files)\n",
            "Processing ca_ca21.txt (25 of 88 files)\n",
            "Processing ca_ca20.txt (26 of 88 files)\n",
            "Processing ca_ca37.txt (27 of 88 files)\n",
            "Processing cb_cb09.txt (28 of 88 files)\n",
            "Processing cc_cc14.txt (29 of 88 files)\n",
            "Processing ca_ca25.txt (30 of 88 files)\n",
            "Processing ca_ca24.txt (31 of 88 files)\n",
            "Processing ca_ca22.txt (32 of 88 files)\n",
            "Processing ca_ca18.txt (33 of 88 files)\n",
            "Processing cc_cc08.txt (34 of 88 files)\n",
            "Processing ca_ca40.txt (35 of 88 files)\n",
            "Processing ca_ca07.txt (36 of 88 files)\n",
            "Processing ca_ca31.txt (37 of 88 files)\n",
            "Processing cb_cb22.txt (38 of 88 files)\n",
            "Processing ca_ca12.txt (39 of 88 files)\n",
            "Processing cc_cc10.txt (40 of 88 files)\n",
            "Processing ca_ca17.txt (41 of 88 files)\n",
            "Processing ca_ca35.txt (42 of 88 files)\n",
            "Processing ca_ca06.txt (43 of 88 files)\n",
            "Processing ca_ca29.txt (44 of 88 files)\n",
            "Processing ca_ca23.txt (45 of 88 files)\n",
            "Processing ca_ca28.txt (46 of 88 files)\n",
            "Processing cb_cb11.txt (47 of 88 files)\n",
            "Processing cc_cc11.txt (48 of 88 files)\n",
            "Processing ca_ca42.txt (49 of 88 files)\n",
            "Processing cb_cb02.txt (50 of 88 files)\n",
            "Processing cb_cb21.txt (51 of 88 files)\n",
            "Processing cb_cb10.txt (52 of 88 files)\n",
            "Processing cb_cb16.txt (53 of 88 files)\n",
            "Processing ca_ca04.txt (54 of 88 files)\n",
            "Processing cc_cc15.txt (55 of 88 files)\n",
            "Processing ca_ca10.txt (56 of 88 files)\n",
            "Processing ca_ca38.txt (57 of 88 files)\n",
            "Processing cb_cb24.txt (58 of 88 files)\n",
            "Processing cb_cb14.txt (59 of 88 files)\n",
            "Processing ca_ca41.txt (60 of 88 files)\n",
            "Processing cb_cb05.txt (61 of 88 files)\n",
            "Processing cc_cc13.txt (62 of 88 files)\n",
            "Processing cc_cc04.txt (63 of 88 files)\n",
            "Processing ca_ca36.txt (64 of 88 files)\n",
            "Processing cc_cc01.txt (65 of 88 files)\n",
            "Processing ca_ca43.txt (66 of 88 files)\n",
            "Processing cc_cc07.txt (67 of 88 files)\n",
            "Processing cb_cb13.txt (68 of 88 files)\n",
            "Processing ca_ca26.txt (69 of 88 files)\n",
            "Processing cb_cb12.txt (70 of 88 files)\n",
            "Processing cb_cb15.txt (71 of 88 files)\n",
            "Processing cb_cb01.txt (72 of 88 files)\n",
            "Processing cb_cb27.txt (73 of 88 files)\n",
            "Processing ca_ca05.txt (74 of 88 files)\n",
            "Processing ca_ca32.txt (75 of 88 files)\n",
            "Processing ca_ca09.txt (76 of 88 files)\n",
            "Processing cb_cb19.txt (77 of 88 files)\n",
            "Processing ca_ca08.txt (78 of 88 files)\n",
            "Processing ca_ca14.txt (79 of 88 files)\n",
            "Processing cb_cb26.txt (80 of 88 files)\n",
            "Processing cb_cb23.txt (81 of 88 files)\n",
            "Processing ca_ca03.txt (82 of 88 files)\n",
            "Processing cb_cb08.txt (83 of 88 files)\n",
            "Processing ca_ca34.txt (84 of 88 files)\n",
            "Processing cc_cc02.txt (85 of 88 files)\n",
            "Processing ca_ca01.txt (86 of 88 files)\n",
            "Processing ca_ca15.txt (87 of 88 files)\n",
            "Processing cb_cb07.txt (88 of 88 files)\n",
            "the\t12711\n",
            "be\t6286\n",
            "of\t6191\n",
            "a\t5446\n",
            "and\t4701\n",
            "to\t4453\n",
            "in\t3837\n",
            "he\t2443\n",
            "have\t1953\n",
            "for\t1810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results1 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single_folder\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"],nhits = 10)\n",
        "for x in conc_results1:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBY7MqfJWjNV",
        "outputId": "b33b9099-564b-4877-eb5b-76c00864c825"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ca_ca16.txt (1 of 88 files)\n",
            "Processing cb_cb25.txt (2 of 88 files)\n",
            "Processing cc_cc03.txt (3 of 88 files)\n",
            "Processing cc_cc16.txt (4 of 88 files)\n",
            "Processing cb_cb04.txt (5 of 88 files)\n",
            "Processing cc_cc05.txt (6 of 88 files)\n",
            "Processing ca_ca39.txt (7 of 88 files)\n",
            "Processing cb_cb03.txt (8 of 88 files)\n",
            "Processing ca_ca13.txt (9 of 88 files)\n",
            "Processing ca_ca19.txt (10 of 88 files)\n",
            "Processing cb_cb20.txt (11 of 88 files)\n",
            "Processing cc_cc06.txt (12 of 88 files)\n",
            "Processing cc_cc17.txt (13 of 88 files)\n",
            "Processing cc_cc09.txt (14 of 88 files)\n",
            "Processing ca_ca11.txt (15 of 88 files)\n",
            "Processing ca_ca30.txt (16 of 88 files)\n",
            "Processing ca_ca02.txt (17 of 88 files)\n",
            "Processing cb_cb06.txt (18 of 88 files)\n",
            "Processing cc_cc12.txt (19 of 88 files)\n",
            "Processing cb_cb18.txt (20 of 88 files)\n",
            "Processing cb_cb17.txt (21 of 88 files)\n",
            "Processing ca_ca33.txt (22 of 88 files)\n",
            "Processing ca_ca27.txt (23 of 88 files)\n",
            "Processing ca_ca44.txt (24 of 88 files)\n",
            "Processing ca_ca21.txt (25 of 88 files)\n",
            "Processing ca_ca20.txt (26 of 88 files)\n",
            "Processing ca_ca37.txt (27 of 88 files)\n",
            "Processing cb_cb09.txt (28 of 88 files)\n",
            "Processing cc_cc14.txt (29 of 88 files)\n",
            "Processing ca_ca25.txt (30 of 88 files)\n",
            "Processing ca_ca24.txt (31 of 88 files)\n",
            "Processing ca_ca22.txt (32 of 88 files)\n",
            "Processing ca_ca18.txt (33 of 88 files)\n",
            "Processing cc_cc08.txt (34 of 88 files)\n",
            "Processing ca_ca40.txt (35 of 88 files)\n",
            "Processing ca_ca07.txt (36 of 88 files)\n",
            "Processing ca_ca31.txt (37 of 88 files)\n",
            "Processing cb_cb22.txt (38 of 88 files)\n",
            "Processing ca_ca12.txt (39 of 88 files)\n",
            "Processing cc_cc10.txt (40 of 88 files)\n",
            "Processing ca_ca17.txt (41 of 88 files)\n",
            "Processing ca_ca35.txt (42 of 88 files)\n",
            "Processing ca_ca06.txt (43 of 88 files)\n",
            "Processing ca_ca29.txt (44 of 88 files)\n",
            "Processing ca_ca23.txt (45 of 88 files)\n",
            "Processing ca_ca28.txt (46 of 88 files)\n",
            "Processing cb_cb11.txt (47 of 88 files)\n",
            "Processing cc_cc11.txt (48 of 88 files)\n",
            "Processing ca_ca42.txt (49 of 88 files)\n",
            "Processing cb_cb02.txt (50 of 88 files)\n",
            "Processing cb_cb21.txt (51 of 88 files)\n",
            "Processing cb_cb10.txt (52 of 88 files)\n",
            "Processing cb_cb16.txt (53 of 88 files)\n",
            "Processing ca_ca04.txt (54 of 88 files)\n",
            "Processing cc_cc15.txt (55 of 88 files)\n",
            "Processing ca_ca10.txt (56 of 88 files)\n",
            "Processing ca_ca38.txt (57 of 88 files)\n",
            "Processing cb_cb24.txt (58 of 88 files)\n",
            "Processing cb_cb14.txt (59 of 88 files)\n",
            "Processing ca_ca41.txt (60 of 88 files)\n",
            "Processing cb_cb05.txt (61 of 88 files)\n",
            "Processing cc_cc13.txt (62 of 88 files)\n",
            "Processing cc_cc04.txt (63 of 88 files)\n",
            "Processing ca_ca36.txt (64 of 88 files)\n",
            "Processing cc_cc01.txt (65 of 88 files)\n",
            "Processing ca_ca43.txt (66 of 88 files)\n",
            "Processing cc_cc07.txt (67 of 88 files)\n",
            "Processing cb_cb13.txt (68 of 88 files)\n",
            "Processing ca_ca26.txt (69 of 88 files)\n",
            "Processing cb_cb12.txt (70 of 88 files)\n",
            "Processing cb_cb15.txt (71 of 88 files)\n",
            "Processing cb_cb01.txt (72 of 88 files)\n",
            "Processing cb_cb27.txt (73 of 88 files)\n",
            "Processing ca_ca05.txt (74 of 88 files)\n",
            "Processing ca_ca32.txt (75 of 88 files)\n",
            "Processing ca_ca09.txt (76 of 88 files)\n",
            "Processing cb_cb19.txt (77 of 88 files)\n",
            "Processing ca_ca08.txt (78 of 88 files)\n",
            "Processing ca_ca14.txt (79 of 88 files)\n",
            "Processing cb_cb26.txt (80 of 88 files)\n",
            "Processing cb_cb23.txt (81 of 88 files)\n",
            "Processing ca_ca03.txt (82 of 88 files)\n",
            "Processing cb_cb08.txt (83 of 88 files)\n",
            "Processing ca_ca34.txt (84 of 88 files)\n",
            "Processing cc_cc02.txt (85 of 88 files)\n",
            "Processing ca_ca01.txt (86 of 88 files)\n",
            "Processing ca_ca15.txt (87 of 88 files)\n",
            "Processing cb_cb07.txt (88 of 88 files)\n",
            "Search returned 115 hits.\n",
            " Returning a random sample of 10 hits\n",
            "[['confirm', 'or', 'deny', 'the', 'reports', 'that', 'he', 'had', 'decided', 'to'], 'run', ['and', 'wanted', 'mr', 'screvane', 'who', 'lives', 'in', 'queens', 'to', 'replace']]\n",
            "[['the', 'milwaukee', 'braves', 'tied', 'a', 'major-league', 'record', 'with', 'fourteen', 'home'], 'runs', ['in', 'three', 'games', 'and', 'lost', 'two', 'of', 'them', 'and', '4']]\n",
            "[['line', 'showed', 'a', 'marked', 'drop', 'of', 'profits', 'on', 'the', 'atlantic'], 'run', ['the', 'cunard', 'line', 'has', 'under', 'consideration', 'replacing', 'the', 'queen', 'mary']]\n",
            "[['the', 'indians', 'bunched', 'three', 'of', 'their', 'eight', 'hits', 'for', 'two'], 'runs', ['in', 'the', 'sixth', 'chuck', 'hinton', 'tripled', 'to', 'the', 'rightfield', 'corner']]\n",
            "[['scene', 'a', 'much', 'more', 'girlish', 'lucia', 'a', 'sensational', 'coloratura', 'who'], 'ran', ['across', 'stage', 'while', 'singing', 'and', 'an', 'actress', 'immersed', 'in', 'her']]\n",
            "[['up', 'gentile', 'can', 'hardly', 'do', 'better', 'than', 'drive', 'in', '98'], 'runs', ['do', 'nt', 'ask', 'him', 'more', 'i', 'have', 'a', 'hunch', 'marv']]\n",
            "[['and', 'became', 'the', 'ninth', 'big', 'leaguer', 'to', 'stroke', 'four', 'home'], 'runs', ['in', 'a', 'game', '3', 'the', 'milwaukee', 'braves', 'tied', 'a', 'major-league']]\n",
            "[['games', 'and', 'mathematically', 'each', 'will', 'hit', 'more', 'than', '60', 'home'], 'runs', ['this', 'is', 'the', 'great', 'edge', 'the', 'two', 'yankees', 'have', 'going']]\n",
            "[['solo', 'blast', 'by', 'bill', 'tuttle', 'tied', 'the', 'game', 'and', 'single'], 'runs', ['in', 'the', 'eighth', 'and', 'ninth', 'gave', 'the', 'athletics', 'their', 'fifth']]\n",
            "[['real', 'after', '108', 'games', 'in', '1927', 'ruth', 'had', '35', 'home'], 'runs', ['after', '108', 'games', 'in', '1961', 'mickey', 'mantle', 'has', '43', 'roger']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results2 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single_folder\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"],collocates = [\"suddenly\", 'just'], nhits = 10)\n",
        "for x in conc_results2:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkJ0MGjOWmzS",
        "outputId": "26cb6b81-301c-4f11-859c-2da83e1399a8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ca_ca16.txt (1 of 88 files)\n",
            "Processing cb_cb25.txt (2 of 88 files)\n",
            "Processing cc_cc03.txt (3 of 88 files)\n",
            "Processing cc_cc16.txt (4 of 88 files)\n",
            "Processing cb_cb04.txt (5 of 88 files)\n",
            "Processing cc_cc05.txt (6 of 88 files)\n",
            "Processing ca_ca39.txt (7 of 88 files)\n",
            "Processing cb_cb03.txt (8 of 88 files)\n",
            "Processing ca_ca13.txt (9 of 88 files)\n",
            "Processing ca_ca19.txt (10 of 88 files)\n",
            "Processing cb_cb20.txt (11 of 88 files)\n",
            "Processing cc_cc06.txt (12 of 88 files)\n",
            "Processing cc_cc17.txt (13 of 88 files)\n",
            "Processing cc_cc09.txt (14 of 88 files)\n",
            "Processing ca_ca11.txt (15 of 88 files)\n",
            "Processing ca_ca30.txt (16 of 88 files)\n",
            "Processing ca_ca02.txt (17 of 88 files)\n",
            "Processing cb_cb06.txt (18 of 88 files)\n",
            "Processing cc_cc12.txt (19 of 88 files)\n",
            "Processing cb_cb18.txt (20 of 88 files)\n",
            "Processing cb_cb17.txt (21 of 88 files)\n",
            "Processing ca_ca33.txt (22 of 88 files)\n",
            "Processing ca_ca27.txt (23 of 88 files)\n",
            "Processing ca_ca44.txt (24 of 88 files)\n",
            "Processing ca_ca21.txt (25 of 88 files)\n",
            "Processing ca_ca20.txt (26 of 88 files)\n",
            "Processing ca_ca37.txt (27 of 88 files)\n",
            "Processing cb_cb09.txt (28 of 88 files)\n",
            "Processing cc_cc14.txt (29 of 88 files)\n",
            "Processing ca_ca25.txt (30 of 88 files)\n",
            "Processing ca_ca24.txt (31 of 88 files)\n",
            "Processing ca_ca22.txt (32 of 88 files)\n",
            "Processing ca_ca18.txt (33 of 88 files)\n",
            "Processing cc_cc08.txt (34 of 88 files)\n",
            "Processing ca_ca40.txt (35 of 88 files)\n",
            "Processing ca_ca07.txt (36 of 88 files)\n",
            "Processing ca_ca31.txt (37 of 88 files)\n",
            "Processing cb_cb22.txt (38 of 88 files)\n",
            "Processing ca_ca12.txt (39 of 88 files)\n",
            "Processing cc_cc10.txt (40 of 88 files)\n",
            "Processing ca_ca17.txt (41 of 88 files)\n",
            "Processing ca_ca35.txt (42 of 88 files)\n",
            "Processing ca_ca06.txt (43 of 88 files)\n",
            "Processing ca_ca29.txt (44 of 88 files)\n",
            "Processing ca_ca23.txt (45 of 88 files)\n",
            "Processing ca_ca28.txt (46 of 88 files)\n",
            "Processing cb_cb11.txt (47 of 88 files)\n",
            "Processing cc_cc11.txt (48 of 88 files)\n",
            "Processing ca_ca42.txt (49 of 88 files)\n",
            "Processing cb_cb02.txt (50 of 88 files)\n",
            "Processing cb_cb21.txt (51 of 88 files)\n",
            "Processing cb_cb10.txt (52 of 88 files)\n",
            "Processing cb_cb16.txt (53 of 88 files)\n",
            "Processing ca_ca04.txt (54 of 88 files)\n",
            "Processing cc_cc15.txt (55 of 88 files)\n",
            "Processing ca_ca10.txt (56 of 88 files)\n",
            "Processing ca_ca38.txt (57 of 88 files)\n",
            "Processing cb_cb24.txt (58 of 88 files)\n",
            "Processing cb_cb14.txt (59 of 88 files)\n",
            "Processing ca_ca41.txt (60 of 88 files)\n",
            "Processing cb_cb05.txt (61 of 88 files)\n",
            "Processing cc_cc13.txt (62 of 88 files)\n",
            "Processing cc_cc04.txt (63 of 88 files)\n",
            "Processing ca_ca36.txt (64 of 88 files)\n",
            "Processing cc_cc01.txt (65 of 88 files)\n",
            "Processing ca_ca43.txt (66 of 88 files)\n",
            "Processing cc_cc07.txt (67 of 88 files)\n",
            "Processing cb_cb13.txt (68 of 88 files)\n",
            "Processing ca_ca26.txt (69 of 88 files)\n",
            "Processing cb_cb12.txt (70 of 88 files)\n",
            "Processing cb_cb15.txt (71 of 88 files)\n",
            "Processing cb_cb01.txt (72 of 88 files)\n",
            "Processing cb_cb27.txt (73 of 88 files)\n",
            "Processing ca_ca05.txt (74 of 88 files)\n",
            "Processing ca_ca32.txt (75 of 88 files)\n",
            "Processing ca_ca09.txt (76 of 88 files)\n",
            "Processing cb_cb19.txt (77 of 88 files)\n",
            "Processing ca_ca08.txt (78 of 88 files)\n",
            "Processing ca_ca14.txt (79 of 88 files)\n",
            "Processing cb_cb26.txt (80 of 88 files)\n",
            "Processing cb_cb23.txt (81 of 88 files)\n",
            "Processing ca_ca03.txt (82 of 88 files)\n",
            "Processing cb_cb08.txt (83 of 88 files)\n",
            "Processing ca_ca34.txt (84 of 88 files)\n",
            "Processing cc_cc02.txt (85 of 88 files)\n",
            "Processing ca_ca01.txt (86 of 88 files)\n",
            "Processing ca_ca15.txt (87 of 88 files)\n",
            "Processing cb_cb07.txt (88 of 88 files)\n",
            "Search returned 1 hits.\n",
            " Returning all 1 hits\n",
            "[['as', 'saying', 'i', 'ca', 'nt', 'hit', 'anymore', 'i', 'ca', 'nt'], 'run', ['i', 'ca', 'nt', 'throw', 'suddenly', 'my', 'reflexes', 'are', 'gone', 'just']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collocates = ct.collocator(ct.tokenize(ct.ldcorpus(\"brown_single_folder\")),\"go\",stat = \"MI\")\n",
        "#stat options include: \"MI\", \"T\", \"freq\", \"left\", and \"right\"\n",
        "\n",
        "ct.head(collocates, hits = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD0vE27HWpcn",
        "outputId": "eaea740b-9caa-401c-d941-07a1c7cc2b7d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ca_ca16.txt (1 of 88 files)\n",
            "Processing cb_cb25.txt (2 of 88 files)\n",
            "Processing cc_cc03.txt (3 of 88 files)\n",
            "Processing cc_cc16.txt (4 of 88 files)\n",
            "Processing cb_cb04.txt (5 of 88 files)\n",
            "Processing cc_cc05.txt (6 of 88 files)\n",
            "Processing ca_ca39.txt (7 of 88 files)\n",
            "Processing cb_cb03.txt (8 of 88 files)\n",
            "Processing ca_ca13.txt (9 of 88 files)\n",
            "Processing ca_ca19.txt (10 of 88 files)\n",
            "Processing cb_cb20.txt (11 of 88 files)\n",
            "Processing cc_cc06.txt (12 of 88 files)\n",
            "Processing cc_cc17.txt (13 of 88 files)\n",
            "Processing cc_cc09.txt (14 of 88 files)\n",
            "Processing ca_ca11.txt (15 of 88 files)\n",
            "Processing ca_ca30.txt (16 of 88 files)\n",
            "Processing ca_ca02.txt (17 of 88 files)\n",
            "Processing cb_cb06.txt (18 of 88 files)\n",
            "Processing cc_cc12.txt (19 of 88 files)\n",
            "Processing cb_cb18.txt (20 of 88 files)\n",
            "Processing cb_cb17.txt (21 of 88 files)\n",
            "Processing ca_ca33.txt (22 of 88 files)\n",
            "Processing ca_ca27.txt (23 of 88 files)\n",
            "Processing ca_ca44.txt (24 of 88 files)\n",
            "Processing ca_ca21.txt (25 of 88 files)\n",
            "Processing ca_ca20.txt (26 of 88 files)\n",
            "Processing ca_ca37.txt (27 of 88 files)\n",
            "Processing cb_cb09.txt (28 of 88 files)\n",
            "Processing cc_cc14.txt (29 of 88 files)\n",
            "Processing ca_ca25.txt (30 of 88 files)\n",
            "Processing ca_ca24.txt (31 of 88 files)\n",
            "Processing ca_ca22.txt (32 of 88 files)\n",
            "Processing ca_ca18.txt (33 of 88 files)\n",
            "Processing cc_cc08.txt (34 of 88 files)\n",
            "Processing ca_ca40.txt (35 of 88 files)\n",
            "Processing ca_ca07.txt (36 of 88 files)\n",
            "Processing ca_ca31.txt (37 of 88 files)\n",
            "Processing cb_cb22.txt (38 of 88 files)\n",
            "Processing ca_ca12.txt (39 of 88 files)\n",
            "Processing cc_cc10.txt (40 of 88 files)\n",
            "Processing ca_ca17.txt (41 of 88 files)\n",
            "Processing ca_ca35.txt (42 of 88 files)\n",
            "Processing ca_ca06.txt (43 of 88 files)\n",
            "Processing ca_ca29.txt (44 of 88 files)\n",
            "Processing ca_ca23.txt (45 of 88 files)\n",
            "Processing ca_ca28.txt (46 of 88 files)\n",
            "Processing cb_cb11.txt (47 of 88 files)\n",
            "Processing cc_cc11.txt (48 of 88 files)\n",
            "Processing ca_ca42.txt (49 of 88 files)\n",
            "Processing cb_cb02.txt (50 of 88 files)\n",
            "Processing cb_cb21.txt (51 of 88 files)\n",
            "Processing cb_cb10.txt (52 of 88 files)\n",
            "Processing cb_cb16.txt (53 of 88 files)\n",
            "Processing ca_ca04.txt (54 of 88 files)\n",
            "Processing cc_cc15.txt (55 of 88 files)\n",
            "Processing ca_ca10.txt (56 of 88 files)\n",
            "Processing ca_ca38.txt (57 of 88 files)\n",
            "Processing cb_cb24.txt (58 of 88 files)\n",
            "Processing cb_cb14.txt (59 of 88 files)\n",
            "Processing ca_ca41.txt (60 of 88 files)\n",
            "Processing cb_cb05.txt (61 of 88 files)\n",
            "Processing cc_cc13.txt (62 of 88 files)\n",
            "Processing cc_cc04.txt (63 of 88 files)\n",
            "Processing ca_ca36.txt (64 of 88 files)\n",
            "Processing cc_cc01.txt (65 of 88 files)\n",
            "Processing ca_ca43.txt (66 of 88 files)\n",
            "Processing cc_cc07.txt (67 of 88 files)\n",
            "Processing cb_cb13.txt (68 of 88 files)\n",
            "Processing ca_ca26.txt (69 of 88 files)\n",
            "Processing cb_cb12.txt (70 of 88 files)\n",
            "Processing cb_cb15.txt (71 of 88 files)\n",
            "Processing cb_cb01.txt (72 of 88 files)\n",
            "Processing cb_cb27.txt (73 of 88 files)\n",
            "Processing ca_ca05.txt (74 of 88 files)\n",
            "Processing ca_ca32.txt (75 of 88 files)\n",
            "Processing ca_ca09.txt (76 of 88 files)\n",
            "Processing cb_cb19.txt (77 of 88 files)\n",
            "Processing ca_ca08.txt (78 of 88 files)\n",
            "Processing ca_ca14.txt (79 of 88 files)\n",
            "Processing cb_cb26.txt (80 of 88 files)\n",
            "Processing cb_cb23.txt (81 of 88 files)\n",
            "Processing ca_ca03.txt (82 of 88 files)\n",
            "Processing cb_cb08.txt (83 of 88 files)\n",
            "Processing ca_ca34.txt (84 of 88 files)\n",
            "Processing cc_cc02.txt (85 of 88 files)\n",
            "Processing ca_ca01.txt (86 of 88 files)\n",
            "Processing ca_ca15.txt (87 of 88 files)\n",
            "Processing cb_cb07.txt (88 of 88 files)\n",
            "wrong\t7.646162657157894\n",
            "far\t5.82740297182804\n",
            "every\t5.664310003868153\n",
            "how\t5.4535175792154975\n",
            "then\t5.423770235821446\n",
            "way\t5.224698888719617\n",
            "before\t5.196129736522846\n",
            "home\t5.018889351578773\n",
            "back\t4.838807735100289\n",
            "after\t4.832381465940856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî®üîß üîç üëÄ Under construction\n",
        "\n",
        "Think about writng code lines for making data frame table using pandas. \n",
        "\n",
        "Note that code lines in the following is from EssentialSteps4TextAnalysis."
      ],
      "metadata": {
        "id": "XzE9vGx9WtIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Tagging (i.e., associating each token with a grammatical category (e.g., mountain - N) )\n",
        "ct.write_corpus(\"tagged_txt\",ct.tag(ct.ldcorpus(\"txtfolder\")))"
      ],
      "metadata": {
        "id": "EZ7XatV0ZpMh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d08b52-40d2-4fd5-a44b-100ee9db69f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing foxtail.txt (1 of 1 files)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Get frequency of your tagged tokens (e.g., POS). 'hits=10' means you want to get the top 10 words. \n",
        "\n",
        "tagged_freq = ct.frequency(ct.reload(\"tagged_txt\"))\n",
        "ct.head(tagged_freq, hits = 10)"
      ],
      "metadata": {
        "id": "TxTrpzkEaXiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ff1497-7a80-469a-83d4-54e638fadb56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 1.txt (1 of 1 files)\n",
            "he_PRON\t16\n",
            "the_DET\t13\n",
            "of_ADP\t13\n",
            "a_DET\t12\n",
            "to_PART\t9\n",
            "and_CCONJ\t9\n",
            "Fox_PROPN\t8\n",
            "tail_NOUN\t8\n",
            "be_AUX\t7\n",
            "have_AUX\t6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí° Now, let's save tagged data as a dataframe and get word clouds!"
      ],
      "metadata": {
        "id": "1DoJZCxobIrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå  Tagged data is in a dictionary format (e.g., {key:value}).\n",
        "type(tagged_freq)"
      ],
      "metadata": {
        "id": "YDjwNH2xbh6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6fc9c33-4244-4dd2-db5c-c0b8b6a73aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Import the pandas package so as to handle dataframe.\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "OvJt1jQBb6KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Generate a dateframe with tagged words (e.g., word_POS) and their frequencies. \n",
        "\n",
        "data_dict = tagged_freq\n",
        "data_items = data_dict.items()\n",
        "data_list = list(data_items)\n",
        "df = pd.DataFrame(data_list)\n",
        "df.columns = [\"Tagged\",\"Freq\"]\n",
        "print(df)"
      ],
      "metadata": {
        "id": "bpGw92j8cRx6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e3d541a-61a8-49f7-c335-c8555d56ae7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Tagged  Freq\n",
            "0        the_DET    13\n",
            "1     √Üsop_PROPN     1\n",
            "2        for_ADP     4\n",
            "3     child_NOUN     1\n",
            "4      Fox_PROPN     8\n",
            "..           ...   ...\n",
            "156  advice_NOUN     1\n",
            "157    seek_VERB     1\n",
            "158   lower_VERB     1\n",
            "159      own_ADJ     1\n",
            "160   level_NOUN     1\n",
            "\n",
            "[161 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí° Splitting tagged columns into Words and POS <font size = '2.3'> part of speech (i.e., grammatical categories)\n",
        "  - e.g., \n",
        "              column          column 1.    column2\n",
        "          yesterday_ADP ‚û°Ô∏è   yesterday       ADP\n",
        "          rain_NOUN             rain         NOUN\n",
        "          yellow_ADJ           yellow.       ADJ"
      ],
      "metadata": {
        "id": "riajmUc-cfJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Codelines to get tagged columns split into words and POS\n",
        "\n",
        "tagged = df[\"Tagged\"]\n",
        "pos = []\n",
        "word = []\n",
        "\n",
        "for i in range(0, len(tagged)):\n",
        "  w = tagged[i]\n",
        "  ws = w.split(\"_\")\n",
        "  word.append(ws[0])\n",
        "  pos.append(ws[1])\n",
        "\n",
        "print(len(tagged))\n",
        "print(word[:10])\n",
        "print(pos[:10])"
      ],
      "metadata": {
        "id": "_Is6HP1udiNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48fa757d-24b8-4ef0-fd31-715a0efc6402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "161\n",
            "['the', '√Üsop', 'for', 'child', 'Fox', 'without', 'a', 'Tail', 'that', 'have']\n",
            "['DET', 'PROPN', 'ADP', 'NOUN', 'PROPN', 'ADP', 'DET', 'PROPN', 'PRON', 'AUX']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üìå Add new columns to the dataframe.\n",
        "\n",
        "df[\"POS\"] = pos\n",
        "df[\"Word\"] = word\n",
        "\n",
        "# Rearranging column order (remove Tagged column)\n",
        "cols = [\"POS\",\"Word\",\"Freq\"]\n",
        "df = df[cols]\n",
        "\n",
        "# Sort by POS and Freq\n",
        "df = df.sort_values(by=['POS', 'Freq'], ascending = False)\n",
        "print(\"Total rows: \", len(df))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "xVE9PFHvfBdm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "249af035-7385-4eda-f028-305b4e806a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows:  161\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     POS   Word  Freq\n",
              "25  VERB   have     4\n",
              "69  VERB    say     4\n",
              "11  VERB  catch     2\n",
              "21  VERB    get     2\n",
              "40  VERB   know     2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9b12a3af-66be-4f62-8636-81dbd7ecc5d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>POS</th>\n",
              "      <th>Word</th>\n",
              "      <th>Freq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>VERB</td>\n",
              "      <td>have</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>VERB</td>\n",
              "      <td>say</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>VERB</td>\n",
              "      <td>catch</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>VERB</td>\n",
              "      <td>get</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>VERB</td>\n",
              "      <td>know</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b12a3af-66be-4f62-8636-81dbd7ecc5d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9b12a3af-66be-4f62-8636-81dbd7ecc5d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9b12a3af-66be-4f62-8636-81dbd7ecc5d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üî®üîß üîç üëÄ Under construction\n",
        "\n",
        "foxtail=\"\"\"The √Üsop for Children The Fox Without a Tail A Fox that had been caught in a trap, succeeded at last, after much painful tugging, in getting away. But he had to leave his beautiful bushy tail behind him.\n",
        "For a long time he kept away from the other Foxes, for he knew well enough that they would all make fun of him and crack jokes and laugh behind his back. But it was hard for him to live alone, and at last he thought of a plan that would perhaps help him out of his trouble.\n",
        "He called a meeting of all the Foxes, saying that he had something of great importance to tell the tribe.\n",
        "When they were all gathered together, the Fox Without a Tail got up and made a long speech about those Foxes who had come to harm because of their tails.\n",
        "This one had been caught by hounds when his tail had become entangled in the hedge. That one had not been able to run fast enough because of the weight of his brush. Besides, it was well known, he said, that men hunt Foxes simply for their tails, which they cut off as prizes of the hunt. With such proof of the danger and uselessness of having a tail, said Master Fox, he would advise every Fox to cut it off, if he valued life and safety.\n",
        "When he had finished talking, an old Fox arose, and said, smiling:\n",
        "\"Master Fox, kindly turn around for a moment, and you shall have your answer.\"\n",
        "When the poor Fox Without a Tail turned around, there arose such a storm of jeers and hooting, that he saw how useless it was to try any longer to persuade the Foxes to part with their tails.\n",
        "Do not listen to the advice of him who seeks to lower you to his own level.\"\"\"\n",
        "\n",
        "text = file.read().replace(\"\\n\", \" \") #Replace lines with spaces.\n",
        "\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,3}\\b') #Getting rid of Stopwords of 1~3 spellings. Regular expression\n",
        "txt = shortword.sub('',foxtail)"
      ],
      "metadata": {
        "id": "OyYk5N6cvcI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown üî®üîß üîç üëÄ Under construction (Failed!)\n",
        "\n",
        "#@markdown üìå Open a txt file. <Use a set of duble quotation marks \"\" and assign the url address as the _url_ variable> \n",
        "url=\"https://raw.githubusercontent.com/ms624atyale/Data_Misc/main/TheAesop_theFoxwithoutaTail.txt\" \n",
        "\n",
        "\n",
        "os.system(\"curl \" + url + \" > foxtail.txt\") #This generates a txt file under the txtfolder directory and moves the whole text of the url to the txt file (e.g., foxtail.txt).  \n",
        "\n",
        "foxtail = open(\"foxtail.txt\")\n",
        "text = file.read().replace(\"\\n\", \" \") #Replace lines with spaces.\n",
        "\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,3}\\b') #Getting rid of Stopwords of 1~3 spellings. Regular expression\n",
        "txt = shortword.sub('',file)\n",
        "\n",
        "file.close() #Close the file you have been working on.\n",
        "\n",
        "#@markdown üìé When you see crimepunish.txt under the Files directory, move it under the txtfolder folder you've created by drag & drop."
      ],
      "metadata": {
        "id": "cn5UjS8xWUpN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
